{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Order Model\n",
    "\n",
    "This is a demonstration of [\"First Order Model\"](https://github.com/AliaksandrSiarohin/first-order-model), which applies animation models to images.\n",
    "\n",
    "**Even on CPU, this will train vor 15-30 minutes**. I hope this makes it accessible to all.\n",
    "\n",
    "### Environment\n",
    "\n",
    "#### General\n",
    "* For video creation, make sure ffmpeg is installed (apt install ffmpeg, brew install ffmpeg, windows idk)\n",
    "* Git needs to be installed for the notebook to clone code\n",
    "\n",
    "**Make sure to run this notebook in the correct environment:**\n",
    "\n",
    "#### Linux and Mac\n",
    "```\n",
    "conda env create -f fom.yml\n",
    "conda activate fom\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "#### Windows\n",
    "```\n",
    "conda env create -f fom_windows.yml\n",
    "conda activate fom\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "If you run into trouble, make sure to *include your OS, OS Version and Python in the bug report in the forum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell only has an effect once: NB_PATH saves the path the notebook is started in\n",
    "# It's important later down for absolute paths to resources in res/ folder\n",
    "from pathlib import Path\n",
    "\n",
    "NB_PATH = None\n",
    "if not NB_PATH:\n",
    "    NB_PATH = Path.cwd()\n",
    "    print(\"Notebook path: {}\".format(NB_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert 'fom' in sys.executable, 'You are not running this notebook in the fom environment'\n",
    "\n",
    "print(\"Running the correct environment.\\nPython interpreter: {}\".format(sys.executable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model on celebrity faces (VoxCeleb)\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "url = 'https://myshare.uni-osnabrueck.de/f/86c0195eb2e74845b77d/?dl=1'\n",
    "filename = NB_PATH / 'res/vox-cpk.pth.tar'\n",
    "\n",
    "def download(url, fname):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    \n",
    "    progress_bar.close()\n",
    "\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong. Please manually delete any residual download files\")\n",
    "    \n",
    "if not filename.exists():\n",
    "    print(\"Downloading Pretrained weight on VOX. Be patient...It's 300mb in size.\")\n",
    "    download(url, filename)\n",
    "\n",
    "print(\"Download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_GPU = torch.cuda.is_available() and torch.cuda.device(torch.cuda.current_device())\n",
    "if USE_GPU:\n",
    "    print(\"Found GPU: {}\".format(torch.cuda.get_device_name(torch.cuda.current_device())))\n",
    "else:\n",
    "    print(\"Using CPU. It will bleed. RIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "<h2>Chose a Driving Video and Source Image in the next section(s)</h2>\n",
    "<table>\n",
    "<tr>\n",
    "  <th>leo.mp4</th>\n",
    "  <th>jon_fookin_snow.png</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "  <video width=\"350\" height=\"350\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"{}\" alt=\"Girl in a jacket\" width=\"350\" height=\"350\">\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th>the_donald.mp4</th>\n",
    "  <th>still.png</th>\n",
    "</tr>\n",
    "<tr>\n",
    "\n",
    "  <td>\n",
    "  <video width=\"350\" height=\"350\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"{}\" alt=\"Girl in a jacket\" width=\"350\" height=\"350\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "\"\"\".format('res/leo.mp4', 'res/jon_fookin_snow.png', 'res/the_donald.mp4', 'res/still.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "The next cell contains configuration options for this notebook\n",
    "\n",
    "Leaving everything unchanged will create a demo with the default driving video and source image. \n",
    "\n",
    "**Change `DRIVING_VIDEO` and `SOURCE_IMAGE` (but leave `RES_PREFIX` untouched) in the `Input Data` Section in the next cell**, if you want to experiment. Map Leo di Caprio onto Jon Snow, for example.\n",
    "\n",
    "There are many options. But the vast majority of them need to stay untouched. You can play with parameters with the `config.yml` file in folder `res`.\n",
    "\n",
    "#### Input Data Extraction\n",
    "If you want to use your own input data (not recommended, you will need additional software), follow [these steps](https://github.com/AliaksandrSiarohin/first-order-model#animation-demo) and adjust the parameters `FIND_BEST_FRAME` and `BEST_FRAME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last check for existing files\n",
    "from pathlib import Path\n",
    "\n",
    "# Prefix to all iamges and videos in /res folder\n",
    "# This needs to be relative, so the HTML elements can load the data\n",
    "RES_PREFIX= Path('res')\n",
    "OUT_PREFIX= Path('out')\n",
    "\n",
    "# Output path, absolute (Note: can't use .absolute() in case working directory changed)\n",
    "OUT=NB_PATH / 'out'\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# Input Data\n",
    "DRIVING_VIDEO=RES_PREFIX / 'the_donald.mp4' # Driving Video to use\n",
    "SOURCE_IMAGE=RES_PREFIX / 'jon_fookin_snow.png' # Source image to use\n",
    "\n",
    "# Configuration\n",
    "CONFIG=NB_PATH / RES_PREFIX / 'config.yml' # Path to model config\n",
    "CKPT=NB_PATH / RES_PREFIX / 'vox-cpk.pth.tar' # path to checkpoint to restore\n",
    "CPU=not USE_GPU\n",
    "\n",
    "# Ouput\n",
    "OUTPUT_VIDEO=OUT_PREFIX / 'result.mp4'\n",
    "OVERWRITE=True # Overwrite existing OUTPUT_VIDEO\n",
    "\n",
    "# Input Data Extraction\n",
    "FIND_BEST_FRAME=False\n",
    "BEST_FRAME=None\n",
    "\n",
    "RELATIVE=True # use relative or absolute keypoint coordinates\n",
    "ADAPT_SCALE=True # adapt movement scale based on convex hull of keypoints\n",
    "\n",
    "def path_assert(Path):\n",
    "    assert Path.exists(), \"{} does not exist!\".format(Path) \n",
    "\n",
    "path_assert(NB_PATH / SOURCE_IMAGE)\n",
    "path_assert(NB_PATH / DRIVING_VIDEO)\n",
    "path_assert(NB_PATH / RES_PREFIX / 'leo.mp4')\n",
    "path_assert(NB_PATH / RES_PREFIX / 'jon_fookin_snow.png')\n",
    "path_assert(NB_PATH / RES_PREFIX / 'the_donald.mp4')\n",
    "path_assert(NB_PATH / RES_PREFIX / 'still.png')\n",
    "path_assert(OUT)\n",
    "path_assert(CONFIG)\n",
    "path_assert(CKPT)\n",
    "if not OVERWRITE and Path(NB_PATH / OUTPUT_VIDEO).exists():\n",
    "    raise Exception(\"Config would overwride existing output file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to determine whether change of working directory has already happened\n",
    "def in_repo():\n",
    "    return Path('sync_batchnorm').exists() and Path('modules').exists() and Path('data').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone necessary code\n",
    "assert(not in_repo())\n",
    "!git clone https://github.com/AliaksandrSiarohin/first-order-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory to git repo (windows supported too, !cd only works in unix based systems )\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# if not in repo, change working directory of notebook\n",
    "if not in_repo():\n",
    "    global NB_PATH, CODE_PATH\n",
    "    NB_PATH = Path.cwd() \n",
    "    CODE_PATH = NB_PATH / 'first-order-model'\n",
    "    os.chdir(CODE_PATH)\n",
    "    print(\"Working directory changed to {}\".format(CODE_PATH))    \n",
    "\n",
    "assert(in_repo() and NB_PATH and CODE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "\n",
    "<h2>You chose</h2>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>Driving Video</th>\n",
    "  <th>Source Image</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "  <video width=\"350\" height=\"350\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"{}\" alt=\"Girl in a jacket\" width=\"350\" height=\"350\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<h4>Run the next cell to map the movement of the driving video onto the source image. The cell after that handles visualizing the results</h4>\n",
    "<div>Note: there will be deprecation warnings. These are normal</div>\n",
    "\n",
    "</div>\n",
    "\"\"\".format(DRIVING_VIDEO, SOURCE_IMAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator\n",
    "from modules.keypoint_detector import KPDetector\n",
    "from animate import normalize_kp\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "ABS_SOURCE_IMAGE = NB_PATH / SOURCE_IMAGE\n",
    "ABS_DRIVING_VIDEO = NB_PATH / DRIVING_VIDEO\n",
    "ABS_OUTPUT_VIDEO = NB_PATH / OUTPUT_VIDEO\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
    "\n",
    "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.load(f)\n",
    "\n",
    "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        generator.cuda()\n",
    "\n",
    "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        kp_detector.cuda()\n",
    "    \n",
    "    if cpu:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    " \n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "    \n",
    "    if not cpu:\n",
    "        generator = DataParallelWithCallback(generator)\n",
    "        kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "    generator.eval()\n",
    "    kp_detector.eval()\n",
    "    \n",
    "    return generator, kp_detector\n",
    "\n",
    "\n",
    "def make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions\n",
    "\n",
    "def find_best_frame(source, driving, cpu=False):\n",
    "    import face_alignment\n",
    "\n",
    "    def normalize_kp(kp):\n",
    "        kp = kp - kp.mean(axis=0, keepdims=True)\n",
    "        area = ConvexHull(kp[:, :2]).volume\n",
    "        area = np.sqrt(area)\n",
    "        kp[:, :2] = kp[:, :2] / area\n",
    "        return kp\n",
    "\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True,\n",
    "                                      device='cpu' if cpu else 'cuda')\n",
    "    kp_source = fa.get_landmarks(255 * source)[0]\n",
    "    kp_source = normalize_kp(kp_source)\n",
    "    norm  = float('inf')\n",
    "    frame_num = 0\n",
    "    for i, image in tqdm(enumerate(driving)):\n",
    "        kp_driving = fa.get_landmarks(255 * image)[0]\n",
    "        kp_driving = normalize_kp(kp_driving)\n",
    "        new_norm = (np.abs(kp_source - kp_driving) ** 2).sum()\n",
    "        if new_norm < norm:\n",
    "            norm = new_norm\n",
    "            frame_num = i\n",
    "    return frame_num\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_image = imageio.imread(ABS_SOURCE_IMAGE)\n",
    "    video_reader = imageio.get_reader(ABS_DRIVING_VIDEO)\n",
    "    fps = video_reader.get_meta_data()['fps']\n",
    "    driving_video = []\n",
    "    try:\n",
    "        for im in video_reader:\n",
    "            driving_video.append(im)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    video_reader.close()\n",
    "\n",
    "    source_image = resize(source_image, (256, 256))[..., :3]\n",
    "    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
    "    \n",
    "    generator, kp_detector = load_checkpoints(config_path=CONFIG, checkpoint_path=CKPT, cpu=CPU)\n",
    "\n",
    "    if FIND_BEST_FRAME or BEST_FRAME is not None:\n",
    "        i = BEST_FRAME if BEST_FRAME is not None else find_best_frame(source_image, driving_video, cpu=CPU)\n",
    "        print (\"Best frame: \" + str(i))\n",
    "        driving_forward = driving_video[i:]\n",
    "        driving_backward = driving_video[:(i+1)][::-1]\n",
    "        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, relative=RELATIVE, adapt_movement_scale=ADAPT_SCALE, cpu=CPU)\n",
    "        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, relative=RELATIVE, adapt_movement_scale=ADAPT_SCALE, cpu=CPU)\n",
    "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
    "    else:\n",
    "        predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=RELATIVE, adapt_movement_scale=ADAPT_SCALE, cpu=CPU)\n",
    "    \n",
    "    imageio.mimsave(ABS_OUTPUT_VIDEO, [img_as_ubyte(frame) for frame in predictions], format='.mp4', fps=fps)\n",
    "    print(\"============================\\nOutput video saved to: {}. Viz result in next cell\".format(ABS_OUTPUT_VIDEO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    " \n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "<table>\n",
    "<tr>\n",
    "  <th>Driving Video</th>\n",
    "  <th>Source Image</th>\n",
    "  <th>Result Video</th>\n",
    "</tr>\n",
    "  <td>\n",
    "  <video width=\"250\" height=\"250\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"{}\" width=\"250\" height=\"250\">\n",
    "  </td>\n",
    "  <td>\n",
    "  <video width=\"250\" height=\"250\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<h5>Note: There might be a slight delay between videos</h5>\n",
    "</div>\n",
    "\"\"\".format(DRIVING_VIDEO, SOURCE_IMAGE, OUTPUT_VIDEO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
