{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone necessary code\n",
    "!git clone https://github.com/AliaksandrSiarohin/first-order-model\n",
    "%cd first-order-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download still image of statue\n",
    "!wget --no-clobber https://myshare.uni-osnabrueck.de/f/2c323b06f1354ed2ae72/?dl=1 -O still.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download video to be mapped onto statue\n",
    "!wget --no-clobber https://myshare.uni-osnabrueck.de/f/43efb4770f54456192ae/?dl=1 -O leo.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model on celebrity faces (VoxCeleb)\n",
    "!wget --no-clobber https://myshare.uni-osnabrueck.de/f/86c0195eb2e74845b77d/?dl=1 -O vox-cpk.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download config file\n",
    "!wget --no-clobber https://myshare.uni-osnabrueck.de/f/41983adf0c4c4fcbabda/?dl=1 -O config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_GPU = torch.cuda.is_available() and torch.cuda.device(torch.cuda.current_device())\n",
    "if USE_GPU:\n",
    "    print(\"Found GPU: {}\".format(torch.cuda.get_device_name(torch.cuda.current_device())))\n",
    "else:\n",
    "    print(\"Using CPU. It will bleed. RIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last check for existing files\n",
    "from pathlib import Path\n",
    "\n",
    "CONF_VALID=False\n",
    "\n",
    "# Input Data\n",
    "CONFIG='config.yml' # Path to model config\n",
    "DRIVING_VIDEO='leo.mp4' # Driving Video to use\n",
    "SOURCE_IMAGE='still.png' # Source image to use\n",
    "CKPT='vox-cpk.pth.tar' # path to checkpoint to restore\n",
    "CPU=not USE_GPU\n",
    "\n",
    "# Ouput\n",
    "OUTPUT_VIDEO='result.mp4'\n",
    "OVERWRITE=True # Overwrite existing OUTPUT_VIDEO\n",
    "\n",
    "# Input Data Extraction\n",
    "FIND_BEST_FRAME=False\n",
    "BEST_FRAME=None\n",
    "\n",
    "RELATIVE=True # use relative or absolute keypoint coordinates\n",
    "ADAPT_SCALE=True # adapt movement scale based on convex hull of keypoints\n",
    "\n",
    "assert(Path(SOURCE_IMAGE).exists())\n",
    "assert(Path(DRIVING_VIDEO).exists())\n",
    "assert(Path(CKPT).exists())\n",
    "assert(Path(CONFIG).exists())\n",
    "if not OVERWRITE and Path(OUTPUT_VIDEO).exists():\n",
    "    raise Exception(\"Config would overwride existing output file!\")\n",
    "    \n",
    "CONF_VALID=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "print(\"  Driving Video (left), Source Image (right)\")\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "  <video width=\"350\" height=\"350\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"{}\" alt=\"Girl in a jacket\" width=\"350\" height=\"350\">\n",
    "  </td>\n",
    "  </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\"\"\".format(DRIVING_VIDEO, SOURCE_IMAGE, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator\n",
    "from modules.keypoint_detector import KPDetector\n",
    "from animate import normalize_kp\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
    "\n",
    "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.load(f)\n",
    "\n",
    "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        generator.cuda()\n",
    "\n",
    "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        kp_detector.cuda()\n",
    "    \n",
    "    if cpu:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    " \n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "    \n",
    "    if not cpu:\n",
    "        generator = DataParallelWithCallback(generator)\n",
    "        kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "    generator.eval()\n",
    "    kp_detector.eval()\n",
    "    \n",
    "    return generator, kp_detector\n",
    "\n",
    "\n",
    "def make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions\n",
    "\n",
    "def find_best_frame(source, driving, cpu=False):\n",
    "    import face_alignment\n",
    "\n",
    "    def normalize_kp(kp):\n",
    "        kp = kp - kp.mean(axis=0, keepdims=True)\n",
    "        area = ConvexHull(kp[:, :2]).volume\n",
    "        area = np.sqrt(area)\n",
    "        kp[:, :2] = kp[:, :2] / area\n",
    "        return kp\n",
    "\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True,\n",
    "                                      device='cpu' if cpu else 'cuda')\n",
    "    kp_source = fa.get_landmarks(255 * source)[0]\n",
    "    kp_source = normalize_kp(kp_source)\n",
    "    norm  = float('inf')\n",
    "    frame_num = 0\n",
    "    for i, image in tqdm(enumerate(driving)):\n",
    "        kp_driving = fa.get_landmarks(255 * image)[0]\n",
    "        kp_driving = normalize_kp(kp_driving)\n",
    "        new_norm = (np.abs(kp_source - kp_driving) ** 2).sum()\n",
    "        if new_norm < norm:\n",
    "            norm = new_norm\n",
    "            frame_num = i\n",
    "    return frame_num\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not CONF_VALID:\n",
    "        raise Exception(\"Sorry, you ran into issues with config and ignored them. Not so fast, bucko.\")\n",
    "    \n",
    "    source_image = imageio.imread(SOURCE_IMAGE)\n",
    "    video_reader = imageio.get_reader(DRIVING_VIDEO)\n",
    "    fps = video_reader.get_meta_data()['fps']\n",
    "    driving_video = []\n",
    "    try:\n",
    "        for im in video_reader:\n",
    "            driving_video.append(im)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    video_reader.close()\n",
    "\n",
    "    source_image = resize(source_image, (256, 256))[..., :3]\n",
    "    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
    "    \n",
    "    generator, kp_detector = load_checkpoints(config_path=CONFIG, checkpoint_path=CKPT, cpu=CPU)\n",
    "\n",
    "    if FIND_BEST_FRAME or BEST_FRAME is not None:\n",
    "        i = BEST_FRAME if BEST_FRAME is not None else find_best_frame(source_image, driving_video, cpu=CPU)\n",
    "        print (\"Best frame: \" + str(i))\n",
    "        driving_forward = driving_video[i:]\n",
    "        driving_backward = driving_video[:(i+1)][::-1]\n",
    "        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, relative=RELATIVE, adapt_movement_scale=ADAPT_SCALE, cpu=CPU)\n",
    "        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, relative=RELATIVE, adapt_movement_scale=ADAPT_SCALE, cpu=CPU)\n",
    "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
    "    else:\n",
    "        predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=RELATIVE, adapt_movement_scale=ADAPT_SCALE, cpu=CPU)\n",
    "    \n",
    "    imageio.mimsave(OUTPUT_VIDEO, [img_as_ubyte(frame) for frame in predictions], format='.mp4', fps=fps)\n",
    "    print(\"Output video saved to: {}\".format(OUTPUT_VIDEO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "print(\"  Driving Video (left), Source Image (right), Result (right)\")\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "  <video width=\"250\" height=\"250\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"{}\" width=\"250\" height=\"250\">\n",
    "  </td>\n",
    "  <td>\n",
    "  <video width=\"250\" height=\"250\" controls autoplay loop>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  </td>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>\n",
    "\"\"\".format(DRIVING_VIDEO, SOURCE_IMAGE, 'result.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
