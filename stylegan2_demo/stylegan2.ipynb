{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU",
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "stylegan2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYKFBOi1H_0N"
      },
      "source": [
        "# StyleGAN2\n",
        "\n",
        "![styleGAN2 generated image sample](https://github.com/sony/nnabla-examples/raw/master/GANs/stylegan2/images/sample.png)\n",
        "\n",
        "\n",
        "This Demo consists of two parts\n",
        "1. A warmup demo that simply generates faces and does style mixing (similar to presentation on friday)\n",
        "2. Exploration of interpolation. You can export the videos, as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn4sVmM66baN"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "# Helper function to download (windows compatibility)\n",
        "def download(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024 #1 Kibibyte\n",
        "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "    \n",
        "    with open(filename, 'wb') as file:\n",
        "        for data in response.iter_content(block_size):\n",
        "            progress_bar.update(len(data))\n",
        "            file.write(data)\n",
        "    \n",
        "    progress_bar.close()\n",
        "\n",
        "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "        print(\"ERROR, something went wrong. Please manually delete any residual download files\")\n",
        "\n",
        "# helper function to change directory (windows compat, !cd doesn't work on windows)\n",
        "def chdir(dir):\n",
        "  os.chdir(dir)\n",
        "  print(\"Changed working directory to: \", dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrBt1A5H_0Y"
      },
      "source": [
        "# Warmup\n",
        "\n",
        "## Preparation\n",
        "If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime â†’ change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start.\n",
        "\n",
        "This cell clones the necessary repo for simple image generation\n",
        "\n",
        "*Note: botocore package warning/errors are expected and can be ignored*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEh42ZcxH_0a"
      },
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "\n",
        "chdir('nnabla-examples/GANs/stylegan2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knjjQIOpH_0c"
      },
      "source": [
        "# Get the pretrained weights\n",
        "Now we will get the pretrained weights for styleGAN2, then import some modules and do some preparation for the latter part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn6lLYcHH_0c"
      },
      "source": [
        "weights = 'styleGAN2_G_params.h5'\n",
        "url = 'https://nnabla.org/pretrained-models/nnabla-examples/GANs/stylegan2/styleGAN2_G_params.h5'\n",
        "download(url, weights)\n",
        "\n",
        "from generate import *\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# init gpu\n",
        "ctx = get_extension_context(\"cudnn\")\n",
        "nn.set_default_context(ctx)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "nn.load_parameters(weights)\n",
        "\n",
        "print(\"Done initializing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc0oz1AGH_0e"
      },
      "source": [
        "# StyleGAN2 input config\n",
        "\n",
        "The noise input **z** is fed to the **mapping network** to produce the latent code **w**. Then **w** is modified via **truncation trick** (*not covered in seminar*) and finally the modified latent code **w'** is injected to the **synthesis network**.\n",
        "\n",
        "With multiple latent codes **w'** coming from the **mapping network**, **synthesis network** transforms the incoming tensor and gradually converts it to an image. \n",
        "\n",
        "\n",
        "In the following cell,  you will choose the random seed used for sampling the noise input **z**, the value for **truncation trick**, and another random seed used for the additional noise input.\n",
        "\n",
        "**Note: if running locally, ignore the markdown marcos @markdown and @param**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWU_8vMXH_0f"
      },
      "source": [
        "#@markdown Choose the seed for noise input **z**. (This drastically changes the result)\n",
        "latent_seed = 300  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the value for truncation trick.\n",
        "truncation_psi = 0.32  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
        "\n",
        "#@markdown Choose the seed for stochasticity input.  (This slightly changes the result)\n",
        "noise_seed = 500  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the seed for stochasticity input.  number of layers to inject noise (a.k.a stochastic variations) into. *This seems to change very little. Default: 18*\n",
        "num_layers = 18 #@param {type: \"slider\", min: 0, max: 500, step:1}\n",
        "\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slZG-KTIH_0g"
      },
      "source": [
        "# Now let's run StyleGAN2 to generate an image (more later)\n",
        "\n",
        "Execution the following cell will run the styleGAN2. You can see by changing the value used for **truncation trick**, you will get the different results.\n",
        "\n",
        "**This cell is to test a single output only, you'll be able to generate way more in a cell further down**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki0SC318H_0g"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "rnd = np.random.RandomState(latent_seed)\n",
        "z = rnd.randn(batch_size, 512)\n",
        "\n",
        "style_noise = nn.Variable((batch_size, 512)).apply(d=z)\n",
        "style_noises = [style_noise for _ in range(num_layers)]\n",
        "\n",
        "rgb_output = generate(batch_size, style_noises, noise_seed, truncation_psi)\n",
        "rgb_output.forward()\n",
        "\n",
        "image = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "filename = f\"seed{latent_seed}.png\"\n",
        "imsave(filename, image, channel_first=True)\n",
        "\n",
        "display(Image(filename, width=512, height=512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctAr6rGDH_0h"
      },
      "source": [
        "# Try Style Mixing\n",
        "\n",
        "![styleGAN2 generated image sample](https://github.com/sony/nnabla-examples/raw/master/GANs/stylegan2/images/style_mixing_sample.png)\n",
        "\n",
        "As described above (and in the presentation), in styleGAN2, **synthesis network** receives latent code **w** multiple times and generates images. In the previous generation, latent code **w** which **synthesis network** receives is made from one single noise input **z**. In this case, we can say that **w** controls the *style* of the generated image.\n",
        "\n",
        "Given that, with a *different* latent code **w2**, made from another noise input **z2**, **synthesis network** can generate a completely different image. So, what if we use both **w** and **w2**...? That is, *style mixing*.\n",
        "\n",
        "To be specific, using 2 latent codes **w** and **w2**, **synthesis network** can generate the image which contains both elements (i.e. hair style, face components), present in images made from **w** (controling coarse style) and **w2** (controling fine style).\n",
        "\n",
        "In the following cell, you will choose one more random seed used for sampling another noise input **z2**. \n",
        "\n",
        "You can also choose from which layer it receives the additional latent code **w2**. It slightly changes the result, so try various patterns.\n",
        "\n",
        "**Note: if running locally, ignore the markdown marcos @markdown and @param**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr3cofJDH_0i"
      },
      "source": [
        "#@title StyleGAN2 style mixing config\n",
        "#@markdown Choose seed for the primary noise input **z**.\n",
        "latent_seed = 300  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose seed for the secondary noise input **z2**.\n",
        "latent_seed2 = 444  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose from which layer to use the secondary latent code **w2**.\n",
        "mix_after = 7  #@param {type: \"slider\", min: 0, max: 17, step:1}\n",
        "\n",
        "#@markdown Choose seed for stochasticity input.\n",
        "noise_seed = 500  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the value for truncation trick.\n",
        "truncation_psi = 0.5  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67un3YKKH_0p"
      },
      "source": [
        "# Let's run style mixing.\n",
        "\n",
        "Running this cell executes style mixing and displays a generated mixed image and images made solely from **w** / **w2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl0IUzpfH_0q"
      },
      "source": [
        "rnd = np.random.RandomState(latent_seed)\n",
        "z = rnd.randn(batch_size, 512)\n",
        "\n",
        "rnd2 = np.random.RandomState(latent_seed2)\n",
        "z2 = rnd2.randn(batch_size, 512)\n",
        "\n",
        "style_noises = [nn.Variable((batch_size, 512)).apply(d=z) for _ in range(mix_after)]\n",
        "style_noises += [nn.Variable((batch_size, 512)).apply(d=z2) for _ in range(num_layers - mix_after)]\n",
        "\n",
        "rgb_output = generate(batch_size, style_noises, noise_seed, truncation_psi)\n",
        "rgb_output.forward()\n",
        "\n",
        "image_mix = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "\n",
        "for style_noise in style_noises:\n",
        "    style_noise.d = z\n",
        "rgb_output.forward()\n",
        "image_A = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "\n",
        "for style_noise in style_noises:\n",
        "    style_noise.d = z2\n",
        "rgb_output.forward()\n",
        "image_B = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "\n",
        "top_image = 255 * np.ones(image_mix.shape).astype(np.uint8)\n",
        "top_image = np.concatenate([top_image, image_B], axis=2)\n",
        "bottom_image = np.concatenate([image_A, image_mix], axis=2)\n",
        "grid_image = np.concatenate([top_image, bottom_image], axis=1)\n",
        "imsave(\"grid.png\", grid_image, channel_first=True)\n",
        "display(Image(\"grid.png\", width=512, height=512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r12J2KN3WJfI"
      },
      "source": [
        "# Exploring Latent Space\n",
        "\n",
        "For this, we need the original code, not wrapped into a library (like the example above)\n",
        "\n",
        "'Config F' will also be downloaded manually (mentioned in the presentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6ie-XLvImdO"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download the code\n",
        "!git clone https://github.com/NVlabs/stylegan2.git\n",
        "chdir('stylegan2')\n",
        "!nvcc test_nvcc.cu -o test_nvcc -run\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__) )\n",
        "!nvidia-smi -L\n",
        "print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXiQhAqIW4cd"
      },
      "source": [
        "## Choose pretrained model\n",
        "\n",
        "Choose between these pretrained models\n",
        "\n",
        "'stylegan2-\\<DATASET\\>-config-f.pkl' is the best choice according to the paper and is downloaded in the cell above! If you chose a different model here, *you must download the weights accordingly in the cell above*\n",
        "\n",
        "Note: **Do not change the default base path 'gdrive:networks' as it depends on your local colab instance and google account**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mljH54z7KOk4"
      },
      "source": [
        "# Download the model of choice\n",
        "import argparse\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "\n",
        "import pretrained_networks\n",
        "\n",
        "# 1024Ã—1024 faces, ffhq stands for FlickrFaceHighQuality\n",
        "# stylegan2-ffhq-config-a.pkl\n",
        "# stylegan2-ffhq-config-b.pkl\n",
        "# stylegan2-ffhq-config-c.pkl\n",
        "# stylegan2-ffhq-config-d.pkl\n",
        "# stylegan2-ffhq-config-e.pkl\n",
        "# stylegan2-ffhq-config-f.pkl\n",
        "\n",
        "# 512Ã—384 cars\n",
        "# stylegan2-car-config-a.pkl\n",
        "# stylegan2-car-config-b.pkl\n",
        "# stylegan2-car-config-c.pkl\n",
        "# stylegan2-car-config-d.pkl\n",
        "# stylegan2-car-config-e.pkl\n",
        "# stylegan2-car-config-f.pkl\n",
        "\n",
        "# 256x256 horses\n",
        "# stylegan2-horse-config-a.pkl\n",
        "# stylegan2-horse-config-f.pkl\n",
        "\n",
        "# 256x256 churches\n",
        "# stylegan2-church-config-a.pkl\n",
        "# stylegan2-church-config-f.pkl\n",
        "\n",
        "# 256x256 cats\n",
        "# stylegan2-cat-config-f.pkl\n",
        "# stylegan2-cat-config-a.pkl\n",
        "network_pkl = \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"\n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "_G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "print('Done initializing model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDOCaiua82n"
      },
      "source": [
        "This cell contains helper functions only!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nLDy9BiKbH3"
      },
      "source": [
        "# Useful utility functions...\n",
        "\n",
        "# Download file\n",
        "def download_gdrive_file(file):\n",
        "  files.download(file) \n",
        "\n",
        "# Generates a list of images, based on a list of latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
        "def generate_images_in_w_space(dlatents, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    Gs_kwargs.truncation_psi = truncation_psi\n",
        "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "\n",
        "    imgs = []\n",
        "    for row, dlatent in log_progress(enumerate(dlatents), name = \"Generating images\"):\n",
        "        #row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(truncation_psi, [-1, 1, 1]) + dlatent_avg\n",
        "        dl = (dlatent-dlatent_avg)*truncation_psi   + dlatent_avg\n",
        "        row_images = Gs.components.synthesis.run(dlatent,  **Gs_kwargs)\n",
        "        imgs.append(PIL.Image.fromarray(row_images[0], 'RGB'))\n",
        "    return imgs       \n",
        "\n",
        "# Generate array of images\n",
        "def generate_images(zs, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    if not isinstance(truncation_psi, list):\n",
        "        truncation_psi = [truncation_psi] * len(zs)\n",
        "        \n",
        "    imgs = []\n",
        "    for z_idx, z in log_progress(enumerate(zs), size = len(zs), name = \"Generating images\"):\n",
        "        Gs_kwargs.truncation_psi = truncation_psi[z_idx]\n",
        "        noise_rnd = np.random.RandomState(1) # fix noise\n",
        "        tflib.set_vars({var: noise_rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
        "        images = Gs.run(z, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "        imgs.append(PIL.Image.fromarray(images[0], 'RGB'))\n",
        "    return imgs\n",
        "\n",
        "# Generate noise input from seeds\n",
        "def generate_zs_from_seeds(seeds):\n",
        "    zs = []\n",
        "    for seed_idx, seed in enumerate(seeds):\n",
        "        rnd = np.random.RandomState(seed)\n",
        "        z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
        "        zs.append(z)\n",
        "    return zs\n",
        "\n",
        "# Generates a list of images, based on a list of seed for latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
        "def generate_images_from_seeds(seeds, truncation_psi):\n",
        "    return generate_images(generate_zs_from_seeds(seeds), truncation_psi)\n",
        "\n",
        "# Save images\n",
        "def saveImgs(imgs, location):\n",
        "  for idx, img in log_progress(enumerate(imgs), size = len(imgs), name=\"Saving images\"):\n",
        "    file = location+ str(idx) + \".png\"\n",
        "    img.save(file)\n",
        "\n",
        "# Show images in matrix\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  a = np.asarray(a, dtype=np.uint8)\n",
        "  str_file = BytesIO()\n",
        "  PIL.Image.fromarray(a).save(str_file, format)\n",
        "  im_data = str_file.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print ('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format)\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def showarray(a, fmt='png'):\n",
        "    a = np.uint8(a)\n",
        "    f = StringIO()\n",
        "    PIL.Image.fromarray(a).save(f, fmt)\n",
        "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "\n",
        "# clamp x with min and max       \n",
        "def clamp(x, minimum, maximum):\n",
        "    return max(minimum, min(x, maximum))\n",
        "    \n",
        "# draw latent \n",
        "def drawLatent(image,latents,x,y,x2,y2, color=(255,0,0,100)):\n",
        "  buffer = PIL.Image.new('RGBA', image.size, (0,0,0,0))\n",
        "   \n",
        "  draw = ImageDraw.Draw(buffer)\n",
        "  cy = (y+y2)/2\n",
        "  draw.rectangle([x,y,x2,y2],fill=(255,255,255,180), outline=(0,0,0,180))\n",
        "  for i in range(len(latents)):\n",
        "    mx = x + (x2-x)*(float(i)/len(latents))\n",
        "    h = (y2-y)*latents[i]*0.1\n",
        "    h = clamp(h,cy-y2,y2-cy)\n",
        "    draw.line((mx,cy,mx,cy+h),fill=color)\n",
        "  return PIL.Image.alpha_composite(image,buffer)\n",
        "             \n",
        "# Create image grid\n",
        "def createImageGrid(images, scale=0.25, rows=1):\n",
        "   w,h = images[0].size\n",
        "   w = int(w*scale)\n",
        "   h = int(h*scale)\n",
        "   height = rows*h\n",
        "   cols = ceil(len(images) / rows)\n",
        "   width = cols*w\n",
        "   canvas = PIL.Image.new('RGBA', (width,height), 'white')\n",
        "   for i,img in enumerate(images):\n",
        "     img = img.resize((w,h), PIL.Image.ANTIALIAS)\n",
        "     canvas.paste(img, (w*(i % cols), h*(i // cols))) \n",
        "   return canvas\n",
        "\n",
        "# Convert noise to mapping network output\n",
        "def convertZtoW(latent, truncation_psi=0.7, truncation_cutoff=9):\n",
        "  dlatent = Gs.components.mapping.run(latent, None) # [seed, layer, component]\n",
        "  dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "  for i in range(truncation_cutoff):\n",
        "    dlatent[0][i] = (dlatent[0][i]-dlatent_avg)*truncation_psi + dlatent_avg\n",
        "    \n",
        "  return dlatent\n",
        "\n",
        "# interpolate between steps with noise\n",
        "def interpolate(zs, steps):\n",
        "   out = []\n",
        "   for i in range(len(zs)-1):\n",
        "    for index in range(steps):\n",
        "     fraction = index/float(steps) \n",
        "     out.append(zs[i+1]*fraction + zs[i]*(1-fraction))\n",
        "   return out\n",
        "\n",
        "# Taken from https://github.com/alexanderkuk/log-progress\n",
        "def log_progress(sequence, every=1, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "\n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "\n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSGMtuQYbI04"
      },
      "source": [
        "## Show images from random seeds!\n",
        "\n",
        "Modify the `n_images` parameter to generate more/fewer images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OnZ5E08L9Lc"
      },
      "source": [
        "# generate some random seeds\n",
        "\n",
        "n_images = 9\n",
        "\n",
        "seeds = np.random.randint(10000000, size=n_images)\n",
        "print('Random seeds: ', seeds)\n",
        "\n",
        "# show the seeds\n",
        "imshow(createImageGrid(generate_images_from_seeds(seeds, 0.7), 0.7 , 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5BL2doPibnW"
      },
      "source": [
        "# Interpolation\n",
        "\n",
        "In the next section, you can choose 2 seeds for random numbers for each image, between which an interpolation is built and saved to a video!\n",
        "\n",
        "**Note: if running locally, ignore the markdown marcos @markdown and @param**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUzS_AWXbryG"
      },
      "source": [
        "#@title StyleGAN2 interpolation config\n",
        "#@markdown Choose seed for the primary noise input **z** (image 1).\n",
        "seed = 5017689  #@param {type: \"slider\", min: 0, max: 10000000, step:1}\n",
        "\n",
        "#@markdown Choose seed for the secondary noise input **z2** (image 2).\n",
        "seed2 = 1941088  #@param {type: \"slider\", min: 0, max: 10000000, step:1}\n",
        "\n",
        "#@markdown Interpolation steps\n",
        "number_of_steps = 20 #@param {type: \"slider\", min: 0, max: 10000, step:1}\n",
        "\n",
        "#@markdown Download resulting video (will be shown below window as well! Choose wisely)\n",
        "save_video = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRLvNFdpMDd0"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Simple (Z) interpolation: interpolation between two input noises\n",
        "zs = generate_zs_from_seeds([seed , seed2])\n",
        "\n",
        "latent1 = zs[0]\n",
        "latent2 = zs[1]\n",
        "\n",
        "imgs = generate_images(interpolate([latent1,latent2],number_of_steps), 1.0)\n",
        "number_of_images = len(imgs)\n",
        "\n",
        "%mkdir -p out\n",
        "\n",
        "movieName = 'out/mov_interpolation.mp4'\n",
        "\n",
        "with imageio.get_writer(movieName, mode='I') as writer:\n",
        "    for image in log_progress(list(imgs), name = \"Creating animation\"):\n",
        "        writer.append_data(np.array(image))\n",
        "\n",
        "if save_video:\n",
        "  print(\"Downloading video\")\n",
        "  download_gdrive_file(movieName)\n",
        "\n",
        "# Display Video. Can't be put into function, because it modifies the cell itself :/\n",
        "mp4 = open(movieName,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEK6ZuQxl48t"
      },
      "source": [
        "## More complex example\n",
        "\n",
        "Interpolating between multiple elements of the style mappings, not noise (one for each image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr968Pntmqm7"
      },
      "source": [
        "#@title StyleGAN2 complex interpolation config\n",
        "\n",
        "#@markdown number of images to interpolate between. You can change the max value if 20 is not enough for your experiments\n",
        "number_of_images = 8 #@param {type: \"slider\", min: 0, max: 20, step:1}\n",
        "\n",
        "#@markdown number of steps of interpolation between pair of two images\n",
        "number_of_steps = 100 #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the value for truncation trick.\n",
        "truncation_psi = 1.0  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
        "\n",
        "#@markdown Download resulting video (will be shown below window as well! Choose wisely)\n",
        "save_video = False #@param {type:\"boolean\"}\n",
        "#@markdown ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6kklektMMfi"
      },
      "source": [
        "# more complex example, interpolating in W instead of Z space.\n",
        "zs = generate_zs_from_seeds(np.random.randint(10000000, size=number_of_images))\n",
        "\n",
        "# It seems my truncation_psi is slightly less efficient in W space - I probably introduced an error somewhere...\n",
        "\n",
        "dls = []\n",
        "for z in zs:\n",
        "  dls.append(convertZtoW(z ,truncation_psi))\n",
        "\n",
        "imgs = generate_images_in_w_space(interpolate(dls,number_of_steps), truncation_psi)\n",
        "\n",
        "%mkdir -p out\n",
        "movieName = 'out/mov_complex.mp4'\n",
        "\n",
        "with imageio.get_writer(movieName, mode='I') as writer:\n",
        "    for image in log_progress(list(imgs), name = \"Creating animation\"):\n",
        "        writer.append_data(np.array(image))\n",
        "\n",
        "if save_video:\n",
        "  download_gdrive_file(movieName)\n",
        "\n",
        "# Display Video. Can't be put into function, because it modifies the cell itself :/\n",
        "mp4 = open(movieName,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}